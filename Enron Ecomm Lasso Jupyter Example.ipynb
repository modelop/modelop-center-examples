{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_smartcomments = \"\"\"\n",
    "#modelop.schema.0: enronEcommInput\n",
    "#modelop.schema.1: enronEcommOutput\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "import gensim\n",
    "import functools\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "def remove_proper_nouns(string):\n",
    "    list_of_words = string.split()\n",
    "    tagged_low = nltk.tag.pos_tag(list_of_words)\n",
    "    removed_proper_nouns = list(filter(lambda x: x[1] != 'NNP', tagged_low))\n",
    "    untagged_low = list(map(lambda x: x[0], removed_proper_nouns))\n",
    "    return \" \".join(untagged_low)\n",
    "\n",
    "def preprocess(series):\n",
    "\n",
    "    removed_proper_nouns = series.astype(str).apply(remove_proper_nouns)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), \n",
    "                      gensim.parsing.preprocessing.strip_tags, \n",
    "                      gensim.parsing.preprocessing.strip_punctuation]\n",
    "\n",
    "    preprocessing = gensim.parsing.preprocess_string\n",
    "    preprocessing_filters = functools.partial(preprocessing, filters=CUSTOM_FILTERS)\n",
    "    removed_punctuation = removed_proper_nouns.apply(preprocessing)\n",
    "\n",
    "    stopword_remover = gensim.parsing.preprocessing.remove_stopwords\n",
    "    stopword_remover_list = lambda x: list(map(stopword_remover, x))\n",
    "    cleaned = removed_punctuation.apply(stopword_remover_list)\n",
    "\n",
    "    filter_short_words = lambda x: list(filter(lambda y: len(y) > 1, x))\n",
    "    cleaned = cleaned.apply(filter_short_words)\n",
    "\n",
    "    filter_non_alpha = lambda x: list(filter(lambda y: y.isalpha(), x))\n",
    "    cleaned = cleaned.apply(filter_non_alpha)\n",
    "    \n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    list_stemmer = lambda x: list(map(lambda y: stemmer.stem(y), x))\n",
    "    cleaned = cleaned.apply(list_stemmer)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def pad_sparse_matrix(sp_mat, length, width):\n",
    "    sp_data = (sp_mat.data, sp_mat.indices, sp_mat.indptr)\n",
    "    padded = scipy.sparse.csr_matrix(sp_data, shape=(length, width))\n",
    "    return padded\n",
    "\n",
    "#modelop.init\n",
    "def begin():\n",
    "    global lasso_model_artifacts \n",
    "    lasso_model_artifacts = pickle.load(open('lasso_model_artifacts.pkl', 'rb'))\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    pass\n",
    "\n",
    "#modelop.score\n",
    "def action(x):\n",
    "    lasso_model = lasso_model_artifacts['lasso_model']\n",
    "    dictionary = lasso_model_artifacts['dictionary']\n",
    "    threshold = lasso_model_artifacts['threshold']\n",
    "    tfidf_model = lasso_model_artifacts['tfidf_model']\n",
    "    \n",
    "    x = pd.DataFrame(x, index=[0])\n",
    "    sys.stdout.flush()\n",
    "    cleaned = preprocess(x.content)\n",
    "    corpus = cleaned.apply(dictionary.doc2bow)\n",
    "    corpus_sparse = gensim.matutils.corpus2csc(corpus).transpose()\n",
    "    corpus_sparse_padded = pad_sparse_matrix(sp_mat = corpus_sparse, \n",
    "                                             length=corpus_sparse.shape[0], \n",
    "                                             width = len(dictionary))\n",
    "    tfidf_vectors = tfidf_model.transform(corpus_sparse_padded)\n",
    "\n",
    "    probabilities = lasso_model.predict_proba(tfidf_vectors)[:,1]\n",
    "\n",
    "    predictions = pd.Series(probabilities > threshold, index=x.index).astype(int)\n",
    "    output = pd.concat([x, predictions], axis=1)\n",
    "    output.columns = ['id', 'content', 'prediction']\n",
    "    output = output.to_dict(orient='records')\n",
    "    yield output\n",
    "\n",
    "def matrix_to_dicts(matrix, labels):\n",
    "    cm = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        cm.append(dict(zip(labels, matrix[idx, :].tolist())))\n",
    "    return cm\n",
    "\n",
    "#modelop.metrics\n",
    "def metrics(x):\n",
    "    lasso_model = lasso_model_artifacts['lasso_model']\n",
    "    dictionary = lasso_model_artifacts['dictionary']\n",
    "    threshold = lasso_model_artifacts['threshold']\n",
    "    tfidf_model = lasso_model_artifacts['tfidf_model']\n",
    "\n",
    "    actuals = x.flagged\n",
    "    \n",
    "    cleaned = preprocess(x.content)\n",
    "    corpus = cleaned.apply(dictionary.doc2bow)\n",
    "    corpus_sparse = gensim.matutils.corpus2csc(corpus).transpose()\n",
    "    corpus_sparse_padded = pad_sparse_matrix(sp_mat = corpus_sparse, \n",
    "                                             length=corpus_sparse.shape[0], \n",
    "                                             width = len(dictionary))\n",
    "    tfidf_vectors = tfidf_model.transform(corpus_sparse_padded)\n",
    "\n",
    "    probabilities = lasso_model.predict_proba(tfidf_vectors)[:,1]\n",
    "\n",
    "    predictions = pd.Series(probabilities > threshold, index=x.index).astype(int) \n",
    "    \n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(actuals, predictions)\n",
    "    fpr,tpr,thres = sklearn.metrics.roc_curve(actuals, predictions)\n",
    "\n",
    "    auc_val = sklearn.metrics.auc(fpr, tpr)\n",
    "    f2_score = sklearn.metrics.fbeta_score(actuals, predictions, beta=2)\n",
    "\n",
    "    roc_curve = [{'fpr': x[0], 'tpr':x[1]} for x in list(zip(fpr, tpr))]\n",
    "    labels = ['Compliant', 'Non-Compliant']\n",
    "    cm = matrix_to_dicts(confusion_matrix, labels)\n",
    "    test_results = dict(roc_curve=roc_curve,\n",
    "                   auc=auc_val,\n",
    "                   f2_score=f2_score,\n",
    "                   confusion_matrix=cm)    \n",
    "\n",
    "    yield test_results\n",
    "\n",
    "def remove_proper_nouns(string):\n",
    "    list_of_words = string.split()\n",
    "    tagged_low = nltk.tag.pos_tag(list_of_words)\n",
    "    removed_proper_nouns = list(filter(lambda x: x[1] != 'NNP', tagged_low))\n",
    "    untagged_low = list(map(lambda x: x[0], removed_proper_nouns))\n",
    "    return \" \".join(untagged_low)\n",
    "\n",
    "#modelop.train\n",
    "def train(data):\n",
    "    y_train = data.flagged\n",
    "    removed_proper_nouns = data.content.astype(str).apply(remove_proper_nouns)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), \n",
    "                  gensim.parsing.preprocessing.strip_tags, \n",
    "                  gensim.parsing.preprocessing.strip_punctuation]\n",
    "    removed_punctuation = removed_proper_nouns.apply(functools.partial(gensim.parsing.preprocess_string, filters=CUSTOM_FILTERS))\n",
    "\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    #Remove stop words, words of length less than 2, and words with non-alphabet characters.\n",
    "    cleaned = removed_punctuation.apply(lambda x: list(map(gensim.parsing.preprocessing.remove_stopwords, x)))\n",
    "    cleaned = cleaned.apply(lambda x: list(filter(lambda y: len(y) > 1, x)))\n",
    "    cleaned = cleaned.apply(lambda x: list(filter(lambda y: y.isalpha(), x)))\n",
    "    cleaned = cleaned.apply(lambda x: list(map(stemmer.stem, x)))\n",
    "\n",
    "    #Create a dictionary (key, value pairs of ids with words which appear in the corpus.\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary(documents=cleaned)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.4)\n",
    "\n",
    "    # Produce a sparse bag-of-words matrix from the word-document frequency counts\n",
    "    corpus = cleaned.apply(dictionary.doc2bow).to_list()\n",
    "    corpus_sparse = gensim.matutils.corpus2csc(corpus).transpose()\n",
    "\n",
    "    # Train a tf-idf transformer and transform the training data\n",
    "    tfidf_model = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "    train_tfidf = tfidf_model.fit_transform(train_corpus_sparse)\n",
    "\n",
    "    # Define and fit a logistic regression model\n",
    "    logreg = sklearn.linear_model.LogisticRegression(penalty='l1', class_weight='balanced', max_iter=2500, random_state=740189)\n",
    "    logreg_model = logreg.fit(X=train_tfidf, y=y_train)\n",
    "\n",
    "    lasso_model_artifacts = dict(lasso_model = logreg_model, \n",
    "                             dictionary = dictionary, \n",
    "                             tfidf_model = train_tfidf, \n",
    "                             threshold = thresh)\n",
    "    pickle.dump(lasso_model_artifacts, open('lasso_model_artifacts.pkl', 'wb'))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input schema\n",
    "{ \n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"email\", \n",
    "  \"fields\":[ \n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    { \"name\": \"content\", \"type\": \"string\"}\n",
    " ] \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output schema\n",
    "{ \n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"email\", \n",
    "  \"fields\":[ \n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    { \"name\": \"content\", \"type\": \"string\"},\n",
    "    {\"name\": \"prediction\", \"type\": \"int\"}\n",
    " ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
