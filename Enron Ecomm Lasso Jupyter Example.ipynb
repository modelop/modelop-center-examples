{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastscore.schema.0: enronEcommInput\n",
    "#fastscore.schema.1: enronEcommOutput\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "import gensim\n",
    "import functools\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "def remove_proper_nouns(string):\n",
    "    list_of_words = string.split()\n",
    "    tagged_low = nltk.tag.pos_tag(list_of_words)\n",
    "    removed_proper_nouns = list(filter(lambda x: x[1] != 'NNP', tagged_low))\n",
    "    untagged_low = list(map(lambda x: x[0], removed_proper_nouns))\n",
    "    return \" \".join(untagged_low)\n",
    "\n",
    "def preprocess(series):\n",
    "\n",
    "    removed_proper_nouns = series.astype(str).apply(remove_proper_nouns)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), \n",
    "                      gensim.parsing.preprocessing.strip_tags, \n",
    "                      gensim.parsing.preprocessing.strip_punctuation]\n",
    "\n",
    "    preprocessing = gensim.parsing.preprocess_string\n",
    "    preprocessing_filters = functools.partial(preprocessing, filters=CUSTOM_FILTERS)\n",
    "    removed_punctuation = removed_proper_nouns.apply(preprocessing)\n",
    "\n",
    "    stopword_remover = gensim.parsing.preprocessing.remove_stopwords\n",
    "    stopword_remover_list = lambda x: list(map(stopword_remover, x))\n",
    "    cleaned = removed_punctuation.apply(stopword_remover_list)\n",
    "\n",
    "    filter_short_words = lambda x: list(filter(lambda y: len(y) > 1, x))\n",
    "    cleaned = cleaned.apply(filter_short_words)\n",
    "\n",
    "    filter_non_alpha = lambda x: list(filter(lambda y: y.isalpha(), x))\n",
    "    cleaned = cleaned.apply(filter_non_alpha)\n",
    "    \n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    list_stemmer = lambda x: list(map(lambda y: stemmer.stem(y), x))\n",
    "    cleaned = cleaned.apply(list_stemmer)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def pad_sparse_matrix(sp_mat, length, width):\n",
    "    sp_data = (sp_mat.data, sp_mat.indices, sp_mat.indptr)\n",
    "    padded = scipy.sparse.csr_matrix(sp_data, shape=(length, width))\n",
    "    return padded\n",
    "\n",
    "#modelop.init\n",
    "def begin():\n",
    "    global lasso_model_artifacts \n",
    "    lasso_model_artifacts = pickle.load(open('lasso_model_artifacts.pkl', 'rb'))\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    pass\n",
    "\n",
    "#modelop.score\n",
    "def action(x):\n",
    "    lasso_model = lasso_model_artifacts['lasso_model']\n",
    "    dictionary = lasso_model_artifacts['dictionary']\n",
    "    threshold = lasso_model_artifacts['threshold']\n",
    "    tfidf_model = lasso_model_artifacts['tfidf_model']\n",
    "    \n",
    "    x = pd.DataFrame(x, index=[0])\n",
    "    print(x)\n",
    "    sys.stdout.flush()\n",
    "    cleaned = preprocess(x.content)\n",
    "    corpus = cleaned.apply(dictionary.doc2bow)\n",
    "    corpus_sparse = gensim.matutils.corpus2csc(corpus).transpose()\n",
    "    corpus_sparse_padded = pad_sparse_matrix(sp_mat = corpus_sparse, \n",
    "                                             length=corpus_sparse.shape[0], \n",
    "                                             width = len(dictionary))\n",
    "    tfidf_vectors = tfidf_model.transform(corpus_sparse_padded)\n",
    "\n",
    "    probabilities = lasso_model.predict_proba(tfidf_vectors)[:,1]\n",
    "\n",
    "    predictions = pd.Series(probabilities > threshold, index=x.index).astype(int)\n",
    "    output = pd.concat([x, predictions], axis=1)\n",
    "    output.columns = ['content', 'id', 'prediction']\n",
    "    output = output.to_dict(orient='records')\n",
    "    print(output)\n",
    "    yield output\n",
    "\n",
    "def matrix_to_dicts(matrix, labels):\n",
    "    cm = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        cm.append(dict(zip(labels, matrix[idx, :].tolist())))\n",
    "    return cm\n",
    "\n",
    "#modelop.metrics\n",
    "def metrics(x):\n",
    "    lasso_model = lasso_model_artifacts['lasso_model']\n",
    "    dictionary = lasso_model_artifacts['dictionary']\n",
    "    threshold = lasso_model_artifacts['threshold']\n",
    "    tfidf_model = lasso_model_artifacts['tfidf_model']\n",
    "\n",
    "    actuals = x.flagged\n",
    "    \n",
    "    cleaned = preprocess(x.content)\n",
    "    corpus = cleaned.apply(dictionary.doc2bow)\n",
    "    corpus_sparse = gensim.matutils.corpus2csc(corpus).transpose()\n",
    "    corpus_sparse_padded = pad_sparse_matrix(sp_mat = corpus_sparse, \n",
    "                                             length=corpus_sparse.shape[0], \n",
    "                                             width = len(dictionary))\n",
    "    tfidf_vectors = tfidf_model.transform(corpus_sparse_padded)\n",
    "\n",
    "    probabilities = lasso_model.predict_proba(tfidf_vectors)[:,1]\n",
    "\n",
    "    predictions = pd.Series(probabilities > threshold, index=x.index).astype(int) \n",
    "    \n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(actuals, predictions)\n",
    "    fpr,tpr,thres = sklearn.metrics.roc_curve(actuals, predictions)\n",
    "\n",
    "    auc_val = sklearn.metrics.auc(fpr, tpr)\n",
    "    f2_score = sklearn.metrics.fbeta_score(actuals, predictions, beta=2)\n",
    "\n",
    "    roc_curve = [{'fpr': x[0], 'tpr':x[1]} for x in list(zip(fpr, tpr))]\n",
    "    labels = ['Compliant', 'Non-Compliant']\n",
    "    cm = matrix_to_dicts(confusion_matrix, labels)\n",
    "    test_results = dict(roc_curve=roc_curve,\n",
    "                   auc=auc_val,\n",
    "                   f2_score=f2_score,\n",
    "                   confusion_matrix=cm)    \n",
    "\n",
    "    yield test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input schema\n",
    "{ \n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"email\", \n",
    "  \"fields\":[ \n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    { \"name\": \"content\", \"type\": \"string\"}\n",
    " ] \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output schema\n",
    "{ \n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"email\", \n",
    "  \"fields\":[ \n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    { \"name\": \"content\", \"type\": \"string\"},\n",
    "    {\"name\": \"prediction\", \"type\": \"int\"}\n",
    " ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
